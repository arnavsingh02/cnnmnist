# -*- coding: utf-8 -*-
"""Another copy of cnnfinal777.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HyC7njaz9stZh-D2MRYJJZTMsiSCND1P
"""

#model.py
import torch
import torch.nn as nn
import torch.nn.functional as F

class MyNeuralNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.Matrix1 = nn.Linear(28**2, 256)
        self.Matrix2 = nn.Linear(256, 128)
        self.Matrix3 = nn.Linear(128, 64)
        self.Matrix4 = nn.Linear(64, 10)
        self.R = nn.ReLU()
        self.dropout = nn.Dropout(0.2)

    def forward(self, x):
        x = x.view(-1, 28**2)
        x = self.R(self.Matrix1(x))
        x = self.dropout(x)
        x = self.R(self.Matrix2(x))
        x = self.dropout(x)
        x = self.R(self.Matrix3(x))
        x = self.dropout(x)
        x = self.Matrix4(x)
        return x.squeeze()

#train.py
import torch
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader, Dataset, random_split
from torch.optim import Adam
import torch.nn.functional as F  # Import functional module
from torch import nn  # Import nn module for neural network functionalities
from model import MyNeuralNet  # Import MyNeuralNet class

# Dataset Class
class MNISTDataset(Dataset):
    def __init__(self, filepath):
        data = pd.read_csv(filepath).values
        self.x = torch.tensor(data[:, 1:].astype(np.float32) / 255.0)
        self.y = torch.tensor(data[:, 0].astype(int))
        self.y_one_hot = F.one_hot(self.y, num_classes=10).float()

    def __len__(self):
        return self.x.shape[0]

    def __getitem__(self, ix):
        return self.x[ix], self.y_one_hot[ix]

# Training Function
def train_model(train_dl, val_dl, model, n_epochs=20):  # Added val_dl
    opt = Adam(model.parameters(), lr=0.001)
    loss_fn = nn.CrossEntropyLoss()
    train_losses = []
    val_losses = []
    train_accuracies = []
    val_accuracies = []

    for epoch in range(n_epochs):
        model.train()  # Set model to training mode
        total_correct_train = 0
        total_samples_train = 0
        epoch_loss_train = 0

        for x, y in train_dl:
            opt.zero_grad()
            y_pred = model(x)
            loss = loss_fn(y_pred, y.argmax(dim=1))
            loss.backward()
            opt.step()

            epoch_loss_train += loss.item()
            total_correct_train += (y_pred.argmax(dim=1) == y.argmax(dim=1)).sum().item()
            total_samples_train += y.size(0)

        train_accuracy = total_correct_train / total_samples_train
        train_losses.append(epoch_loss_train / len(train_dl))
        train_accuracies.append(train_accuracy)

        # Validation Loop
        model.eval()  # Set model to evaluation mode
        with torch.no_grad():  # Disable gradient calculation for validation
            total_correct_val = 0
            total_samples_val = 0
            epoch_loss_val = 0

            for x, y in val_dl:
                y_pred = model(x)
                loss = loss_fn(y_pred, y.argmax(dim=1))

                epoch_loss_val += loss.item()
                total_correct_val += (y_pred.argmax(dim=1) == y.argmax(dim=1)).sum().item()
                total_samples_val += y.size(0)

        val_accuracy = total_correct_val / total_samples_val
        val_losses.append(epoch_loss_val / len(val_dl))
        val_accuracies.append(val_accuracy)

        print(f'Epoch {epoch+1}: Train Loss={train_losses[-1]:.4f}, Train Accuracy={train_accuracy*100:.2f}%, Val Loss={val_losses[-1]:.4f}, Val Accuracy={val_accuracy*100:.2f}%')

    return model, train_losses, val_losses, train_accuracies, val_accuracies

# Load training data from Colab's sample data
full_dataset = MNISTDataset('/content/sample_data/mnist_train_small.csv')

# Split the dataset into training and validation sets (e.g., 80% train, 20% validation)
train_size = int(0.8 * len(full_dataset))
val_size = len(full_dataset) - train_size
train_ds, val_ds = random_split(full_dataset, [train_size, val_size])

train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)
val_dl = DataLoader(val_ds, batch_size=64, shuffle=False)  # No need to shuffle validation data

# Initialize and train the model
model = MyNeuralNet()  # Now this should work correctly
trained_model, train_losses, val_losses, train_accuracies, val_accuracies = train_model(train_dl, val_dl, model)

# Save the model state dict (optional if you want to save it for later use)
torch.save(trained_model.state_dict(), 'mnist_model.pth')

# Visualizations: Loss vs Epoch Graph
plt.figure(figsize=(10, 5))
plt.plot(train_losses, label='Training Loss')
plt.plot(val_losses, label='Validation Loss')
plt.title('Loss vs Epoch')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid()
plt.show()

# Visualizations: Accuracy vs Epoch Graph
plt.figure(figsize=(10, 5))
plt.plot(train_accuracies, label='Training Accuracy')
plt.plot(val_accuracies, label='Validation Accuracy')
plt.title('Accuracy vs Epoch')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid()
plt.show()

#inference.py
import torch
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader
import seaborn as sns
from model import MyNeuralNet  # Ensure this imports your model class
from train import MNISTDataset  # Ensure this imports your dataset class

# Load the dataset for inference from Colab's sample data
test_ds = MNISTDataset('/content/sample_data/mnist_test.csv')
test_dl = DataLoader(test_ds, batch_size=64, shuffle=False)

# Load the trained model (if needed again)
model = MyNeuralNet()
model.load_state_dict(torch.load('mnist_model.pth'))
model.eval()

# Run Inference on Test Data
all_preds = []
all_labels = []

with torch.no_grad():
    for xs, ys in test_dl:
        yhats = model(xs).argmax(dim=1)
        all_preds.append(yhats)
        all_labels.append(ys.argmax(dim=1))

all_preds_tensor = torch.cat(all_preds)
all_labels_tensor = torch.cat(all_labels)

# Function to compute confusion matrix manually
def compute_confusion_matrix(y_true, y_pred, num_classes=10):
    cm = torch.zeros(num_classes, num_classes, dtype=torch.int64)
    for t, p in zip(y_true.view(-1), y_pred.view(-1)):
        cm[t.item(), p.item()] += 1
    return cm

# Function to plot confusion matrix
def plot_confusion_matrix(cm):
    plt.figure(figsize=(10, 7))
    sns.heatmap(cm.numpy(), annot=True, fmt='d', cmap='Blues', xticklabels=np.arange(10), yticklabels=np.arange(10))
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.title('Confusion Matrix')
    plt.show()

# Calculate confusion matrix and F1 score manually
confusion_mat = compute_confusion_matrix(all_labels_tensor, all_preds_tensor)
plot_confusion_matrix(confusion_mat)

# Manual calculation of F1 Score (Macro F1 Score)
def calculate_f1_score(y_true, y_pred):
    tp = (y_true * y_pred).sum().float()
    fp = ((1 - y_true) * y_pred).sum().float()
    fn = (y_true * (1 - y_pred)).sum().float()

    precision = tp / (tp + fp) if tp + fp > 0 else 0.0
    recall = tp / (tp + fn) if tp + fn > 0 else 0.0

    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0.0
    return f1_score

# Calculate F1 Score for each class and average it (Macro F1 Score)
f1_scores = []
for i in range(10):
    binary_labels = (all_labels_tensor == i).float()
    binary_preds = (all_preds_tensor == i).float()
    f1_scores.append(calculate_f1_score(binary_labels, binary_preds))

macro_f1_score = sum(f1_scores) / len(f1_scores)
print(f'F1 Score (Macro): {macro_f1_score:.4f}')

# Incorrect Predictions Visualization
incorrect_indices = (all_preds_tensor != all_labels_tensor).nonzero(as_tuple=True)[0]
fig, ax = plt.subplots(5, 5, figsize=(10, 10))
for i in range(25):
    idx = incorrect_indices[i].item()
    ax[i//5, i%5].imshow(test_ds.x[idx].reshape(28, 28), cmap='gray')
    ax[i//5, i%5].set_title(f'True: {all_labels_tensor[idx].item()}, Pred: {all_preds_tensor[idx].item()}')
    ax[i//5, i%5].axis('off')
plt.tight_layout()
plt.show()